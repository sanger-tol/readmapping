/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    sanger-tol/readmapping Nextflow base config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

process {

    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 5
    maxErrors     = '-1'

    // In this configuration file, we give little resources by default and
    // explicitly bump them up for some processes.
    // All rules should still increase resources every attempt to allow the
    // pipeline to self-heal from MEMLIMIT/RUNLIMIT.

    // Default
    cpus   = 1
    memory = { check_max( 50.MB * task.attempt, 'memory' ) }
    time   = { check_max( 30.min * task.attempt, 'time' ) }

    withName: 'SAMTOOLS_(CONVERT)' {
        time   = { check_max( 1.hour * task.attempt, 'time' ) }
    }

    withName: 'SAMTOOLS_(STATS)' {
        // Actually less than 1 hour for PacBio HiFi data, but confirmed 3 hours for Hi-C
        time   = { check_max( 4.hour * task.attempt, 'time' ) }
    }

    withName: 'SAMTOOLS_(COLLATETOFASTA|COLLATETOFASTQ|FILTERTOFASTQ|FIXMATE|FLAGSTAT|MARKDUP|MERGE|VIEW)' {
        time   = { check_max( 8.hour * task.attempt, 'time' ) }
    }

    withName: 'SAMTOOLS_(FLAGSTAT|IDXSTATS)' {
        memory = { check_max( 250.MB  * task.attempt, 'memory'  ) }
    }

    withName: 'SAMTOOLS_(STATS|VIEW)' {
        memory = { check_max( ((meta.datatype == "pacbio_clr" || meta.datatype == "ont") ? 2.GB : 1.GB) * task.attempt, 'memory'  ) }
    }

    // minimum 1GB memory
    withName: 'BBMAP_FILTERBYNAME' {
        memory = { check_max( 1.GB * task.attempt, 'memory' ) }
    }

    withName: 'SAMTOOLS_COLLATETOFASTA' {
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
        memory = { check_max( 1.GB  * Math.ceil( meta.read_count / 1000000 ) * task.attempt, 'memory' ) }
    }

    withName: 'SAMTOOLS_COLLATETOFASTQ' {
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
        memory = { check_max( 250.MB + 1.GB  * Math.ceil( meta.read_count / 100000000 ) * task.attempt, 'memory' ) }
    }

    withName: 'SAMTOOLS_SORMADUP' {
        cpus   = { log_increase_cpus(2, 6*task.attempt, 1, 2) }
        memory = { check_max( 4.GB + 850.MB * log_increase_cpus(2, 6*task.attempt, 1, 2) * task.attempt + 0.6.GB * Math.ceil( meta.read_count / 100000000 ), 'memory' ) }
        time   = { check_max( 2.h * Math.ceil( meta.read_count / 100000000 ) * task.attempt / log_increase_cpus(2, 6*task.attempt, 1, 2), 'time' ) }
    }

    withName: BLAST_BLASTN {
        time   = { check_max(          2.hour  * Math.ceil( meta.read_count / 1000000 ) * task.attempt, 'time'   ) }
        memory = { check_max( 100.MB + 20.MB   * Math.ceil( meta.read_count / 1000000 ) * task.attempt, 'memory' ) }
        // The tool never seems to use more than 1 core even when given multiple. Sticking to 1 (the default)
    }

    withName: BWAMEM2_INDEX {
        memory = { check_max( 24.GB  * Math.ceil( meta.genome_size / 1000000000 ) * task.attempt, 'memory' ) }
        time   = { check_max( 30.min * Math.ceil( meta.genome_size / 1000000000 ) * task.attempt, 'time'   ) }
        // Not multithreaded
    }

    withName: BWAMEM2_MEM {
        // Corresponds to 12 threads as the minimum, 24 threads if 3 billion reads
        cpus   = { log_increase_cpus(6, 6*task.attempt, meta.read_count/1000000000, 2) }
        // Runtime for 1 billion reads on 12 threads is a function of the logarithm of the genome size
        // Runtime is considered proportional to the number of reads and inversely to number of threads
        time   = { check_max( 3.h * task.attempt *  Math.ceil(positive_log(meta2.genome_size/100000, 10)) * Math.ceil(meta.read_count/1000000000) * 12 / log_increase_cpus(6, 6*task.attempt, meta.read_count/1000000000, 2), 'time' ) }
        // Base RAM usage is about 6 times the genome size.
        // Each thread takes an additional 800 MB RAM for bwa-mem2 and 800 MB for samtools sort
        memory = { check_max( 8.GB + 6.GB * Math.ceil(meta2.genome_size / 1000000000) + 1600.MB * task.attempt * log_increase_cpus(6, 6*task.attempt, meta.read_count/1000000000, 2), 'memory' ) }
    }

    withName: '.*:ALIGN_HIFI:MINIMAP2_ALIGN' {
        cpus   = { log_increase_cpus(4, 2*task.attempt, meta.read_count/1000000, 2) }
        memory = { check_max( 800.MB * log_increase_cpus(4, 2*task.attempt, meta.read_count/1000000, 2) + 14.GB * Math.ceil( Math.pow(meta2.genome_size / 1000000000, 0.6)) * task.attempt, 'memory' ) }
        time   = { check_max(        3.h  * Math.ceil( meta.read_count   / 1000000   ) * task.attempt, 'time'   ) }
    }

    // Extrapolated from the HIFI settings on the basis of 1 ONT alignment. CLR assumed to behave the same way as ONT
    withName: '.*:ALIGN_(CLR|ONT):MINIMAP2_ALIGN' {
        cpus   = { log_increase_cpus(4, 2*task.attempt, meta.read_count/1000000, 2) }
        memory = { check_max( 800.MB * log_increase_cpus(4, 2*task.attempt, meta.read_count/1000000, 2) + 30.GB * Math.ceil( Math.pow(meta2.genome_size / 1000000000, 0.6)) * task.attempt, 'memory' ) }
        time   = { check_max(        1.h  * Math.ceil( meta.read_count   / 1000000   ) * task.attempt, 'time'   ) }
    }

    withName: CRUMBLE {
        // Most genomes seem happy with 1 GB, but some need a bit more, and others a lot more.
        // There is a weak, but existing, correlation between memory usage and the number of reads.
        // The formula below is a quadratic growth based on the attempt number.
        memory = { check_max( task.attempt * (task.attempt + 1) * 512.MB * Math.ceil( meta.read_count / 1000000000 ), 'memory' ) }
        // 100k reads per hour for PacBio, 50m for HiC
        time   = { check_max( 1.h  * Math.ceil( meta.read_count / (meta.datatype == "pacbio" ? 100000 : 50000000)) * task.attempt, 'time'   ) }
    }

    withName:CUSTOM_DUMPSOFTWAREVERSIONS {
        cache = false
    }
}
